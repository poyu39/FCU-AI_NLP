{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bae131e",
      "metadata": {
        "id": "2bae131e"
      },
      "outputs": [],
      "source": [
        "\n",
        "################################\n",
        "##              ##\n",
        "## 載入與了解IMDB網路電影資料集 ##\n",
        "##              ##\n",
        "################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f760af1a",
      "metadata": {
        "id": "f760af1a"
      },
      "outputs": [],
      "source": [
        "\n",
        "##載入tensorflow做使用\n",
        "##tensorflow的tf.keras.dataset.imdb已內建imdb的資料集\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2188080b",
      "metadata": {
        "id": "2188080b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d17076b9",
      "metadata": {
        "id": "d17076b9"
      },
      "outputs": [],
      "source": [
        "\n",
        "##load_data函數內的參數num_words設定變數top_words，表示要取出資料集中前多少個最常出現的單字，以上面指令而言，就是取出前1,000個單字。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30527106",
      "metadata": {
        "id": "30527106"
      },
      "outputs": [],
      "source": [
        "top_words =1000\n",
        "(train_x,train_y), (test_x,test_y) = tf.keras.datasets.imdb.load_data(num_words=top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dae41ca",
      "metadata": {
        "id": "6dae41ca"
      },
      "outputs": [],
      "source": [
        "\n",
        "##下載後會將資料集的訓練與測試資料分別儲存在(train_x, train_y)、(test_x,test_y)中\n",
        "##可透過shape指令顯示訓練和測試資料集內各維度的資料數量（也稱為形狀），顯示各訓練與測試資料集的資料數量都是25,000筆：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c51c50",
      "metadata": {
        "id": "a2c51c50",
        "outputId": "ca9fb5f5-2b7e-404a-e4f9-8c1f4d93af69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_x's shape:'(25000,)\n",
            "train_y's shape:'(25000,)\n",
            "test_x's shape:'(25000,)\n",
            "test_y's shape:'(25000,)\n"
          ]
        }
      ],
      "source": [
        "print(\"train_x's shape:'{0}\".format(train_x.shape))\n",
        "print(\"train_y's shape:'{0}\".format(train_y.shape))\n",
        "print(\"test_x's shape:'{0}\".format(test_x.shape))\n",
        "print(\"test_y's shape:'{0}\".format(test_y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff64264",
      "metadata": {
        "id": "9ff64264"
      },
      "outputs": [],
      "source": [
        "\n",
        "##也可檢視訓練資料集內第1筆評論資料（矩陣索引值從0起算），及其對應的標籤資料：\n",
        "##下面data的輸出結果顯示為一個整數值的矩陣，這是因為該評論的單字已置換成「單字 - 索引」（Word-index）\n",
        "##而該索引對應到單字 - 索引字典。標籤的部分，整數1表示正面評價，0表示負面評價。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44f86304",
      "metadata": {
        "id": "44f86304",
        "outputId": "3ee20e69-75f1-43c8-9104-a5c14cb8df0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data:'[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "label:'1\n"
          ]
        }
      ],
      "source": [
        "print(\"data:'{0}\".format(train_x[0]))\n",
        "print(\"label:'{0}\".format(train_y[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb0a69b3",
      "metadata": {
        "id": "fb0a69b3"
      },
      "outputs": [],
      "source": [
        "\n",
        "##若欲顯示回原始文字，可透過下面技巧進行解碼，將整數陣列顯示回單字。首先利用imdb模組中的get_word_index()函數取得單字索引字典：\n",
        "##試看看he這個詞後面接的index以及word是什麼?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac42fae",
      "metadata": {
        "id": "dac42fae"
      },
      "outputs": [],
      "source": [
        "words_mapping = tf.keras.datasets.imdb.get_word_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a1bf5d",
      "metadata": {
        "id": "e8a1bf5d",
        "outputId": "ab52e11d-9c93-4bf4-c490-54f050784271"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word [he]'s index is:26\n"
          ]
        }
      ],
      "source": [
        "print(\"word [he]'s index is:{0}\".format(words_mapping[\"he\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e1dff4d",
      "metadata": {
        "id": "9e1dff4d"
      },
      "outputs": [],
      "source": [
        "\n",
        "##接著可以用下面技巧製作「索引 - 單字」（Index-words）字典：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a725c947",
      "metadata": {
        "id": "a725c947"
      },
      "outputs": [],
      "source": [
        "indice_mapping = dict([(value,key) for (key,value) in words_mapping.items()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2799e58",
      "metadata": {
        "id": "f2799e58",
        "outputId": "cddf44d6-7e70-4dbd-a655-cf35f6cec459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "index [400]'s word is:name\n",
            "index [317]'s word is:half\n"
          ]
        }
      ],
      "source": [
        "print(\"index [400]'s word is:{0}\".format(indice_mapping[400]))\n",
        "print(\"index [317]'s word is:{0}\".format(indice_mapping[317]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e66b7c24",
      "metadata": {
        "id": "e66b7c24"
      },
      "outputs": [],
      "source": [
        "\n",
        "##最後就可以將訓練資料集第 1 筆評論資料顯示回單字\n",
        "##解碼後的結果中，可看到內容中含有一些「？」，表示該單字沒對應的索引。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b077906",
      "metadata": {
        "id": "1b077906",
        "outputId": "c307aa53-e0d5-4376-ab09-8938e9786e56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"? this film was just brilliant casting ? ? story direction ? really ? the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same ? ? as myself so i loved the fact there was a real ? with this film the ? ? throughout the film were great it was just brilliant so much that i ? the film as soon as it was released for ? and would recommend it to everyone to watch and the ? ? was amazing really ? at the end it was so sad and you know what they say if you ? at a film it must have been good and this definitely was also ? to the two little ? that played the ? of ? and paul they were just brilliant children are often left out of the ? ? i think because the stars that play them all ? up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don't you think the whole story was so ? because it was true and was ? life after all that was ? with us all\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def decode_review(target):\n",
        "    return (\" \".join([indice_mapping.get(i-3,\"?\") for i in target]))\n",
        "decode_review(train_x[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34064584",
      "metadata": {
        "id": "34064584"
      },
      "outputs": [],
      "source": [
        "\n",
        "###############\n",
        "##           ##\n",
        "## 資料預處理 ##\n",
        "##           ##\n",
        "###############\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33866135",
      "metadata": {
        "id": "33866135"
      },
      "outputs": [],
      "source": [
        "\n",
        "##為了便於神經網路的訓練，接下來要先對評論的內容執行預處理。\n",
        "##由於我們會將評論以批次的方式傳入輸入層進行神經網路訓練，因此必須先將各筆評論的長度（也就是單字數量）填充或剪裁成相同長度。\n",
        "##首先檢視前10筆評論內容的長度，從輸出結果可以發現各則評論的字數皆不相同：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2477ae1a",
      "metadata": {
        "id": "2477ae1a",
        "outputId": "50721358-9a14-45ee-9019-4fc9978df4a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "218\n",
            "189\n",
            "141\n",
            "550\n",
            "147\n",
            "43\n",
            "123\n",
            "562\n",
            "233\n",
            "130\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print(len(train_x[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d3731dd",
      "metadata": {
        "id": "9d3731dd"
      },
      "outputs": [],
      "source": [
        "\n",
        "##接著利用TensorFlow.keras.preprocessing模組提供的”sequence”函式處理長度，參數的設定如下說明：\n",
        "##  1. sequences：欲處理的資料集，也就是訓練資料集或測試資料集。\n",
        "##  2. max_len_words：每筆資料的單字數，會將每筆資料依情況填充或剪裁以符合此設定數量，在這次的範例中，我們將評論內容長度設定為100。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67070d4",
      "metadata": {
        "id": "b67070d4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "max_len_word=100\n",
        "train_x = sequence.pad_sequences(sequences=train_x, maxlen=max_len_word)\n",
        "test_x = sequence.pad_sequences(sequences=test_x, maxlen=max_len_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "204e8764",
      "metadata": {
        "id": "204e8764"
      },
      "outputs": [],
      "source": [
        "\n",
        "##執行後可以透過shape查看各維度的資料數量，確認25,000個評論的長度皆為100：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67c062a7",
      "metadata": {
        "id": "67c062a7",
        "outputId": "6d96aa9e-05bd-4263-e31d-0104fc23e3d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_x's shape:(25000, 100)\n",
            "test_x's shape:(25000, 100)\n"
          ]
        }
      ],
      "source": [
        "print(\"train_x's shape:{0}\".format(train_x.shape))\n",
        "print(\"test_x's shape:{0}\".format(test_x.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45492290",
      "metadata": {
        "id": "45492290"
      },
      "outputs": [],
      "source": [
        "\n",
        "#############\n",
        "##         ##\n",
        "## 建構RNN ##\n",
        "##         ##\n",
        "#############\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f5b653",
      "metadata": {
        "id": "53f5b653"
      },
      "outputs": [],
      "source": [
        "\n",
        "##首先創建RNN模型，模型利用\n",
        "## 1.TensorFlow.keras.models\n",
        "## 2.TensorFlow.keras.layers\n",
        "## 兩個模組所提供的函式來創建：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53bbd6f2",
      "metadata": {
        "id": "53bbd6f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "#下面指令首先創建一個Sequential模型，然後加入Embedding層。\n",
        "#這層主要負責將評論中每個單字的整數索引轉換為「固定長度的詞向量」，以範例的設定而言，就是將每一個單字的整數索引以維度32的向量來表示。\n",
        "#請注意，此層一定是Sequential模型的第一層。\n",
        "#Embedding()函式第1個參數 input_dim設定為特徵的單字數量，也就是一開始設定最常用的1,000個單字\n",
        "#第2個參數output_dim為輸出詞向量的維度\n",
        "#第3個參數input_length 為輸入的評論內容長度，即長度100。\n",
        "#最後使用print(model.summary())查詢目前定義的模型架構\n",
        "\n",
        "#接著就加入SimpleRNN層，參數設置為Embedding層的輸出維度，最後再加入Dense層，因為結果是正面評價或負面評價，因此只需單顆神經元即可表示。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d9f4b9",
      "metadata": {
        "id": "18d9f4b9",
        "outputId": "0ca3f52c-72ec-4191-c5bb-becd9d2fceb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 32)           32000     \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34,113\n",
            "Trainable params: 34,113\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,SimpleRNN,Dense\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=top_words,output_dim=32,input_length=max_len_word))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='relu'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09f7bf99",
      "metadata": {
        "id": "09f7bf99"
      },
      "outputs": [],
      "source": [
        "\n",
        "#模型建立完成後，接著就是編譯模型、訓練模型，並進行評估的操作：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cbae7bc",
      "metadata": {
        "id": "6cbae7bc"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d2682da",
      "metadata": {
        "id": "4d2682da",
        "outputId": "990d0471-31d2-45b5-c607-2c8261536024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "157/157 - 3s - loss: 0.9548 - accuracy: 0.5353 - val_loss: 0.6687 - val_accuracy: 0.5870 - 3s/epoch - 19ms/step\n",
            "Epoch 2/20\n",
            "157/157 - 2s - loss: 0.6344 - accuracy: 0.6332 - val_loss: 0.6241 - val_accuracy: 0.6590 - 2s/epoch - 14ms/step\n",
            "Epoch 3/20\n",
            "157/157 - 2s - loss: 0.5685 - accuracy: 0.7124 - val_loss: 0.6180 - val_accuracy: 0.6906 - 2s/epoch - 15ms/step\n",
            "Epoch 4/20\n",
            "157/157 - 2s - loss: 0.5098 - accuracy: 0.7717 - val_loss: 0.7667 - val_accuracy: 0.5894 - 2s/epoch - 14ms/step\n",
            "Epoch 5/20\n",
            "157/157 - 2s - loss: 0.6256 - accuracy: 0.6515 - val_loss: 0.6539 - val_accuracy: 0.6116 - 2s/epoch - 14ms/step\n",
            "Epoch 6/20\n",
            "157/157 - 2s - loss: 0.5827 - accuracy: 0.6957 - val_loss: 0.6588 - val_accuracy: 0.6408 - 2s/epoch - 14ms/step\n",
            "Epoch 7/20\n",
            "157/157 - 2s - loss: 0.5601 - accuracy: 0.7245 - val_loss: 0.6447 - val_accuracy: 0.6734 - 2s/epoch - 14ms/step\n",
            "Epoch 8/20\n",
            "157/157 - 2s - loss: 0.4849 - accuracy: 0.7768 - val_loss: 0.5960 - val_accuracy: 0.7484 - 2s/epoch - 15ms/step\n",
            "Epoch 9/20\n",
            "157/157 - 2s - loss: 0.8812 - accuracy: 0.7151 - val_loss: 0.7450 - val_accuracy: 0.5958 - 2s/epoch - 14ms/step\n",
            "Epoch 10/20\n",
            "157/157 - 2s - loss: 0.6049 - accuracy: 0.6730 - val_loss: 0.6938 - val_accuracy: 0.6054 - 2s/epoch - 14ms/step\n",
            "Epoch 11/20\n",
            "157/157 - 2s - loss: 0.5661 - accuracy: 0.7031 - val_loss: 0.6952 - val_accuracy: 0.6206 - 2s/epoch - 14ms/step\n",
            "Epoch 12/20\n",
            "157/157 - 2s - loss: 0.5419 - accuracy: 0.7229 - val_loss: 0.6948 - val_accuracy: 0.6334 - 2s/epoch - 14ms/step\n",
            "Epoch 13/20\n",
            "157/157 - 2s - loss: 0.5174 - accuracy: 0.7369 - val_loss: 0.7076 - val_accuracy: 0.6508 - 2s/epoch - 14ms/step\n",
            "Epoch 14/20\n",
            "157/157 - 2s - loss: 0.4975 - accuracy: 0.7516 - val_loss: 0.7305 - val_accuracy: 0.6568 - 2s/epoch - 14ms/step\n",
            "Epoch 15/20\n",
            "157/157 - 2s - loss: 0.4752 - accuracy: 0.7622 - val_loss: 0.7725 - val_accuracy: 0.6676 - 2s/epoch - 14ms/step\n",
            "Epoch 16/20\n",
            "157/157 - 2s - loss: 0.4513 - accuracy: 0.7800 - val_loss: 0.7975 - val_accuracy: 0.6760 - 2s/epoch - 14ms/step\n",
            "Epoch 17/20\n",
            "157/157 - 2s - loss: 0.4243 - accuracy: 0.7965 - val_loss: 0.8395 - val_accuracy: 0.6910 - 2s/epoch - 14ms/step\n",
            "Epoch 18/20\n",
            "157/157 - 2s - loss: 0.3920 - accuracy: 0.8159 - val_loss: 0.8495 - val_accuracy: 0.7078 - 2s/epoch - 14ms/step\n",
            "Epoch 19/20\n",
            "157/157 - 2s - loss: 0.3702 - accuracy: 0.8364 - val_loss: 0.8640 - val_accuracy: 0.7474 - 2s/epoch - 14ms/step\n",
            "Epoch 20/20\n",
            "157/157 - 2s - loss: 0.3744 - accuracy: 0.8357 - val_loss: 0.7583 - val_accuracy: 0.7290 - 2s/epoch - 14ms/step\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_x,train_y,validation_split=0.2,epochs=20, batch_size=128,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b81230",
      "metadata": {
        "id": "64b81230"
      },
      "outputs": [],
      "source": [
        "\n",
        "#從評估結果可以看到，驗證準確度為77%，循環神經網路表現不是很好，這是因為RNN的記憶力沒有這麼好，處理太長的序列資料會遺忘早期輸入的資料。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcd057eb",
      "metadata": {
        "id": "dcd057eb",
        "outputId": "d77f6a0e-48c1-4e9e-948e-bd76f96e25fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7692 - accuracy: 0.7288\n"
          ]
        }
      ],
      "source": [
        "loss ,accuracy = model.evaluate(test_x,test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c42359",
      "metadata": {
        "id": "36c42359"
      },
      "outputs": [],
      "source": [
        "\n",
        "#############\n",
        "##         ##\n",
        "## 建構LSTM ##\n",
        "##         ##\n",
        "#############\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "353ede95",
      "metadata": {
        "id": "353ede95"
      },
      "outputs": [],
      "source": [
        "\n",
        "##接著將模型改為LSTM，實作上只要先載入LSTM層模組，接著仿照RNN建立模型的部分，並將RNN層替換成LSTM層即可，其他部分都以類似的邏輯撰寫：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d64333",
      "metadata": {
        "id": "d4d64333",
        "outputId": "1033a4ff-a5fb-48c8-cbd4-f8504edb8459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 32)           32000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40,353\n",
            "Trainable params: 40,353\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.layers import Embedding,SimpleRNN,Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim=top_words,output_dim=32,input_length=max_len_word))\n",
        "lstm_model.add(LSTM(32))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "print(lstm_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e937222",
      "metadata": {
        "id": "8e937222"
      },
      "outputs": [],
      "source": [
        "lstm_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb3abdb",
      "metadata": {
        "id": "cfb3abdb",
        "outputId": "635b759a-ce2a-4c6c-9f10-5182ed457287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "157/157 - 7s - loss: 0.5494 - accuracy: 0.7291 - val_loss: 0.4309 - val_accuracy: 0.8046 - 7s/epoch - 45ms/step\n",
            "Epoch 2/20\n",
            "157/157 - 5s - loss: 0.4124 - accuracy: 0.8152 - val_loss: 0.4173 - val_accuracy: 0.8118 - 5s/epoch - 35ms/step\n",
            "Epoch 3/20\n",
            "157/157 - 5s - loss: 0.3832 - accuracy: 0.8296 - val_loss: 0.4082 - val_accuracy: 0.8122 - 5s/epoch - 34ms/step\n",
            "Epoch 4/20\n",
            "157/157 - 5s - loss: 0.3702 - accuracy: 0.8363 - val_loss: 0.4525 - val_accuracy: 0.7976 - 5s/epoch - 35ms/step\n",
            "Epoch 5/20\n",
            "157/157 - 5s - loss: 0.3618 - accuracy: 0.8409 - val_loss: 0.3960 - val_accuracy: 0.8194 - 5s/epoch - 34ms/step\n",
            "Epoch 6/20\n",
            "157/157 - 5s - loss: 0.3539 - accuracy: 0.8460 - val_loss: 0.4228 - val_accuracy: 0.8048 - 5s/epoch - 35ms/step\n",
            "Epoch 7/20\n",
            "157/157 - 5s - loss: 0.3476 - accuracy: 0.8484 - val_loss: 0.4798 - val_accuracy: 0.8042 - 5s/epoch - 35ms/step\n",
            "Epoch 8/20\n",
            "157/157 - 5s - loss: 0.3414 - accuracy: 0.8502 - val_loss: 0.3813 - val_accuracy: 0.8286 - 5s/epoch - 35ms/step\n",
            "Epoch 9/20\n",
            "157/157 - 5s - loss: 0.3383 - accuracy: 0.8506 - val_loss: 0.3917 - val_accuracy: 0.8254 - 5s/epoch - 35ms/step\n",
            "Epoch 10/20\n",
            "157/157 - 5s - loss: 0.3306 - accuracy: 0.8515 - val_loss: 0.3880 - val_accuracy: 0.8186 - 5s/epoch - 31ms/step\n",
            "Epoch 11/20\n",
            "157/157 - 5s - loss: 0.3241 - accuracy: 0.8580 - val_loss: 0.4074 - val_accuracy: 0.8232 - 5s/epoch - 32ms/step\n",
            "Epoch 12/20\n",
            "157/157 - 5s - loss: 0.3185 - accuracy: 0.8601 - val_loss: 0.4042 - val_accuracy: 0.8210 - 5s/epoch - 32ms/step\n",
            "Epoch 13/20\n",
            "157/157 - 5s - loss: 0.3166 - accuracy: 0.8640 - val_loss: 0.4401 - val_accuracy: 0.8062 - 5s/epoch - 31ms/step\n",
            "Epoch 14/20\n",
            "157/157 - 5s - loss: 0.3090 - accuracy: 0.8651 - val_loss: 0.3946 - val_accuracy: 0.8262 - 5s/epoch - 31ms/step\n",
            "Epoch 15/20\n",
            "157/157 - 5s - loss: 0.3049 - accuracy: 0.8659 - val_loss: 0.3982 - val_accuracy: 0.8178 - 5s/epoch - 31ms/step\n",
            "Epoch 16/20\n",
            "157/157 - 5s - loss: 0.3013 - accuracy: 0.8692 - val_loss: 0.3990 - val_accuracy: 0.8160 - 5s/epoch - 31ms/step\n",
            "Epoch 17/20\n",
            "157/157 - 5s - loss: 0.2984 - accuracy: 0.8701 - val_loss: 0.4325 - val_accuracy: 0.8074 - 5s/epoch - 31ms/step\n",
            "Epoch 18/20\n",
            "157/157 - 5s - loss: 0.2912 - accuracy: 0.8712 - val_loss: 0.4328 - val_accuracy: 0.8212 - 5s/epoch - 32ms/step\n",
            "Epoch 19/20\n",
            "157/157 - 5s - loss: 0.2877 - accuracy: 0.8734 - val_loss: 0.4078 - val_accuracy: 0.8176 - 5s/epoch - 32ms/step\n",
            "Epoch 20/20\n",
            "157/157 - 5s - loss: 0.2847 - accuracy: 0.8780 - val_loss: 0.4049 - val_accuracy: 0.8212 - 5s/epoch - 31ms/step\n"
          ]
        }
      ],
      "source": [
        "lstm_history = lstm_model.fit(train_x,train_y,validation_split=0.2, epochs=20, batch_size=128, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5bbee6",
      "metadata": {
        "id": "8f5bbee6",
        "outputId": "cae0aa95-de13-465e-f65c-ff063f038618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 4s 5ms/step - loss: 0.3904 - accuracy: 0.8280\n"
          ]
        }
      ],
      "source": [
        "lstm_loss, lstm_accuracy = lstm_model.evaluate(test_x,test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e22083",
      "metadata": {
        "id": "04e22083"
      },
      "outputs": [],
      "source": [
        "\n",
        "########################\n",
        "##                    ##\n",
        "## 執行測試集的評論分類 ##\n",
        "##                    ##\n",
        "########################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3e110ca",
      "metadata": {
        "id": "d3e110ca"
      },
      "outputs": [],
      "source": [
        "\n",
        "##最後就可用此模型執行測試資料集的評論分類：\n",
        "##下半部順便定義兩個方便使用的函式，一個是 display_test_sentiment() 函式，裡面首先會呼叫第二個函式get_original_text()\n",
        "##因此會先顯示目前要分類的評論內容，接著才顯示此評論的分類答案，以及模型分類的結果。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebce6567",
      "metadata": {
        "id": "ebce6567"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "predict=model.predict(test_x)\n",
        "predict=np.argmax(predict,axis=1)\n",
        "predict_class=predict.reshape(len(test_x))\n",
        "\n",
        "\n",
        "def get_original_text(i):\n",
        "    word_to_id = tf.keras.datasets.imdb.get_word_index()\n",
        "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
        "    word_to_id[\"<PAD>\"] =0\n",
        "    word_to_id[\"<START>\"] =1\n",
        "    word_to_id[\"UNK\"] =2\n",
        "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "    return ' '.join(id_to_word[id] for id in test_x[i])\n",
        "\n",
        "SemtimentDcit={1:'positive',0:'negative'}\n",
        "def display_test_sentiment(content_index):\n",
        "    print(get_original_text(i))\n",
        "    print('label:',SemtimentDcit[test_y[i]], ',prediction:',SemtimentDcit[predict_class[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df8d72c6",
      "metadata": {
        "id": "df8d72c6",
        "outputId": "3451486c-d08b-4394-8d32-fa1aa0430619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNK UNK the UNK side of UNK UNK br br there are some UNK UNK in this film about the only UNK i can really point out is a certain to the script in some UNK which i think is due mostly to the way this film is a four UNK fight there simply isn't enough time to UNK UNK what's going on br br UNK this is a UNK good film i highly recommend watching this in UNK with the first and then UNK for how good the series could have been had it UNK under UNK and UNK\n",
            "label: positive ,prediction: negative\n"
          ]
        }
      ],
      "source": [
        "display_test_sentiment(content_index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15781be2",
      "metadata": {
        "id": "15781be2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}