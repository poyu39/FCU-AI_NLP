{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A6kXAawsnhOG"
      },
      "outputs": [],
      "source": [
        "\n",
        "####################################\n",
        "##                ##\n",
        "## 載入與了解IMDB網路電影資料集 ##\n",
        "##                ##\n",
        "####################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "flJlR-einiAx"
      },
      "outputs": [],
      "source": [
        "\n",
        "##載入tensorflow做使用\n",
        "##tensorflow的tf.keras.dataset.imdb已內建imdb的資料集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCv85re0njxU",
        "outputId": "2033e347-c9af-4543-c9b2-c73f85e8babd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "top_words =1000\n",
        "(train_x,train_y), (test_x,test_y) = tf.keras.datasets.imdb.load_data(num_words=top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsrgRRbWnlEl",
        "outputId": "da05d063-519f-4e54-8b23-5f91f6725c1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_x's shape:'(25000,)\n",
            "train_y's shape:'(25000,)\n",
            "test_x's shape:'(25000,)\n",
            "test_y's shape:'(25000,)\n"
          ]
        }
      ],
      "source": [
        "##下載後會將資料集的訓練與測試資料分別儲存在(train_x, train_y)、(test_x,test_y)中\n",
        "##可透過shape指令顯示訓練和測試資料集內各維度的資料數量（也稱為形狀），顯示各訓練與測試資料集的資料數量都是25,000筆：\n",
        "print(\"train_x's shape:'{0}\".format(train_x.shape))\n",
        "print(\"train_y's shape:'{0}\".format(train_y.shape))\n",
        "print(\"test_x's shape:'{0}\".format(test_x.shape))\n",
        "print(\"test_y's shape:'{0}\".format(test_y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAQaTdSfnqLa",
        "outputId": "b535ed26-4832-46ec-edd8-f629745696d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data:'[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "label:'1\n"
          ]
        }
      ],
      "source": [
        "##也可檢視訓練資料集內第1筆評論資料（矩陣索引值從0起算），及其對應的標籤資料：\n",
        "##下面data的輸出結果顯示為一個整數值的矩陣，這是因為該評論的單字已置換成「單字 - 索引」（Word-index）\n",
        "##而該索引對應到單字 - 索引字典。標籤的部分，整數1表示正面評價，0表示負面評價。\n",
        "print(\"data:'{0}\".format(train_x[0]))\n",
        "print(\"label:'{0}\".format(train_y[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Xvx_3__Anv0f"
      },
      "outputs": [],
      "source": [
        "\n",
        "##################\n",
        "##       ##\n",
        "## 資料預處理 ##\n",
        "##       ##\n",
        "##################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O60iAZ5Unzi9"
      },
      "outputs": [],
      "source": [
        "##  會將每筆資料依情況填充或剪裁以符合此設定數量，在這次的範例中，我們將評論內容長度設定為100。\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "max_len_word=100\n",
        "train_x = sequence.pad_sequences(sequences=train_x, maxlen=max_len_word)\n",
        "test_x = sequence.pad_sequences(sequences=test_x, maxlen=max_len_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx2EGrhOn_Zt",
        "outputId": "2e0ac334-f75a-4210-fac0-03e90ab7026c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_x's shape:(25000, 100)\n",
            "test_x's shape:(25000, 100)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "##執行後可以透過shape查看各維度的資料數量，確認25,000個評論的長度皆為100：\n",
        "print(\"train_x's shape:{0}\".format(train_x.shape))\n",
        "print(\"test_x's shape:{0}\".format(test_x.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uxXqWP6SoB8V"
      },
      "outputs": [],
      "source": [
        "\n",
        "################\n",
        "##      ##\n",
        "## 建構Model ##\n",
        "##      ##\n",
        "################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PDcbP-xBoEJD"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGY1_mgZoGbB",
        "outputId": "02184048-6966-4370-d3b1-e3012cb063da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               12928     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 264       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17329 (67.69 KB)\n",
            "Trainable params: 17329 (67.69 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(units=128, input_dim=100, activation='relu'))\n",
        "model.add(Dense(units=32))\n",
        "model.add(Dense(units=8))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KGKXBfKKoIU3"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZXN27dMoJj7",
        "outputId": "2a2924b7-4a78-487e-cdce-bd23856c345a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1250/1250 - 6s - loss: 5.0958 - accuracy: 0.4976 - 6s/epoch - 5ms/step\n",
            "Epoch 2/30\n",
            "1250/1250 - 2s - loss: 0.9494 - accuracy: 0.5059 - 2s/epoch - 2ms/step\n",
            "Epoch 3/30\n",
            "1250/1250 - 2s - loss: 0.7631 - accuracy: 0.5141 - 2s/epoch - 2ms/step\n",
            "Epoch 4/30\n",
            "1250/1250 - 2s - loss: 0.7173 - accuracy: 0.5252 - 2s/epoch - 2ms/step\n",
            "Epoch 5/30\n",
            "1250/1250 - 2s - loss: 0.6991 - accuracy: 0.5419 - 2s/epoch - 2ms/step\n",
            "Epoch 6/30\n",
            "1250/1250 - 4s - loss: 0.6870 - accuracy: 0.5518 - 4s/epoch - 3ms/step\n",
            "Epoch 7/30\n",
            "1250/1250 - 3s - loss: 0.6766 - accuracy: 0.5671 - 3s/epoch - 2ms/step\n",
            "Epoch 8/30\n",
            "1250/1250 - 4s - loss: 0.6661 - accuracy: 0.5803 - 4s/epoch - 3ms/step\n",
            "Epoch 9/30\n",
            "1250/1250 - 2s - loss: 0.6532 - accuracy: 0.5935 - 2s/epoch - 2ms/step\n",
            "Epoch 10/30\n",
            "1250/1250 - 4s - loss: 0.6401 - accuracy: 0.6074 - 4s/epoch - 3ms/step\n",
            "Epoch 11/30\n",
            "1250/1250 - 2s - loss: 0.6240 - accuracy: 0.6247 - 2s/epoch - 2ms/step\n",
            "Epoch 12/30\n",
            "1250/1250 - 2s - loss: 0.6040 - accuracy: 0.6405 - 2s/epoch - 2ms/step\n",
            "Epoch 13/30\n",
            "1250/1250 - 2s - loss: 0.5870 - accuracy: 0.6564 - 2s/epoch - 2ms/step\n",
            "Epoch 14/30\n",
            "1250/1250 - 2s - loss: 0.5746 - accuracy: 0.6678 - 2s/epoch - 2ms/step\n",
            "Epoch 15/30\n",
            "1250/1250 - 3s - loss: 0.5622 - accuracy: 0.6761 - 3s/epoch - 2ms/step\n",
            "Epoch 16/30\n",
            "1250/1250 - 3s - loss: 0.5455 - accuracy: 0.6867 - 3s/epoch - 2ms/step\n",
            "Epoch 17/30\n",
            "1250/1250 - 2s - loss: 0.5336 - accuracy: 0.6964 - 2s/epoch - 2ms/step\n",
            "Epoch 18/30\n",
            "1250/1250 - 2s - loss: 0.5240 - accuracy: 0.7038 - 2s/epoch - 2ms/step\n",
            "Epoch 19/30\n",
            "1250/1250 - 2s - loss: 0.5135 - accuracy: 0.7092 - 2s/epoch - 2ms/step\n",
            "Epoch 20/30\n",
            "1250/1250 - 2s - loss: 0.4989 - accuracy: 0.7183 - 2s/epoch - 2ms/step\n",
            "Epoch 21/30\n",
            "1250/1250 - 3s - loss: 0.4955 - accuracy: 0.7249 - 3s/epoch - 3ms/step\n",
            "Epoch 22/30\n",
            "1250/1250 - 2s - loss: 0.4836 - accuracy: 0.7298 - 2s/epoch - 2ms/step\n",
            "Epoch 23/30\n",
            "1250/1250 - 2s - loss: 0.4717 - accuracy: 0.7371 - 2s/epoch - 2ms/step\n",
            "Epoch 24/30\n",
            "1250/1250 - 2s - loss: 0.4656 - accuracy: 0.7427 - 2s/epoch - 2ms/step\n",
            "Epoch 25/30\n",
            "1250/1250 - 2s - loss: 0.4574 - accuracy: 0.7480 - 2s/epoch - 2ms/step\n",
            "Epoch 26/30\n",
            "1250/1250 - 3s - loss: 0.4524 - accuracy: 0.7514 - 3s/epoch - 2ms/step\n",
            "Epoch 27/30\n",
            "1250/1250 - 2s - loss: 0.4448 - accuracy: 0.7552 - 2s/epoch - 2ms/step\n",
            "Epoch 28/30\n",
            "1250/1250 - 2s - loss: 0.4373 - accuracy: 0.7596 - 2s/epoch - 2ms/step\n",
            "Epoch 29/30\n",
            "1250/1250 - 2s - loss: 0.4310 - accuracy: 0.7644 - 2s/epoch - 2ms/step\n",
            "Epoch 30/30\n",
            "1250/1250 - 2s - loss: 0.4278 - accuracy: 0.7665 - 2s/epoch - 2ms/step\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_x,train_y,epochs=30, batch_size=20,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "J7ekBfOQoKgt"
      },
      "outputs": [],
      "source": [
        "\n",
        "#######################\n",
        "\n",
        "# 建構Autoencoder_dnn\n",
        "\n",
        "#######################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vd0k1Q2_oRGs"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 參數設定\n",
        "max_length = 100      # 句子的最大長度\n",
        "encoding_dim = 16     # 編碼的目標維度\n",
        "\n",
        "# 定義自編碼器模型\n",
        "input_layer = Input(shape=(max_length,))  # 輸入層維度是句子長度\n",
        "\n",
        "# 編碼部分\n",
        "\n",
        "\n",
        "# 解碼部分\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "autoencoder_dnn = Model(input_layer, decoded)\n",
        "autoencoder_dnn.compile(optimizer='adam', loss='mse')  # 使用均方誤差作為損失函數\n",
        "autoencoder_dnn.summary()\n",
        "autoencoder_dnn.fit(train_x, train_x, epochs=10, batch_size=32, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EpjHNZtsorMu"
      },
      "outputs": [],
      "source": [
        "#有什麼問題呢? 該怎麼修改?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "w3Wu4U46ov40"
      },
      "outputs": [],
      "source": [
        "#利用encoder的句子進行類似word2vec的轉換，最終將結果丟給簡易的深度學習模型進行預測"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "E1WSkuCJpJk7"
      },
      "outputs": [],
      "source": [
        "\n",
        "#######################\n",
        "\n",
        "# 建構Autoencoder_lstm\n",
        "\n",
        "#######################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sEXQZw5IpOhT"
      },
      "outputs": [],
      "source": [
        "#利用lstm進行autoencoder的訓練，效果會不會比dnn來的好?\n",
        "#模型設計在encoder與encoder裡面各有一層lstm，其中在什麼時候我需要使用many to many的傳遞參數?\n",
        "#sparse_categorical_crossentropy與categorical_crossentropy的差別是什麼?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msy2OL47q7G6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
